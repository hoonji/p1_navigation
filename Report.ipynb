{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Navigation\n",
    "\n",
    "This project solves the single agent Banana Collector Unity environment.\n",
    "\n",
    "## Environment details\n",
    "\n",
    "The Banana Collector environment involves collecting bananas in a large, square world.\n",
    "\n",
    "Each yellow banana collected gives +1 reward, and each blue banana gives -1.\n",
    "\n",
    "There are 37 continuous dimensions in the state space including the agent's velocity, direction, and ray-based object perception.\n",
    "\n",
    "The action space consists of four discrete values: move forward, move backward, turn left, and turn right.\n",
    "\n",
    "The environment is considered solved when the agent gets a score of +13 over 100 consecutive episodes.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "The code borrows heavily from the code in the Deep Q-Networks lesson.\n",
    "\n",
    "The code trains a DQN agent to solve the banana collection environment. The agent's model is a simple deep neural network consisting of two fully connected layers with 64 units each and relu activation. The agent learns by drawing batches of experiences (S, A, R, S' tuples) of size 64 from its replay buffer. This feature prevents learning from being skewed by sequential correlations between experiences, which limits oscillation and divergence.\n",
    "\n",
    "The agent also learns using fixed Q-targets. To implement this, a separate target network is maintained to compute TD targets. This networks parameters are updated very slowly (only .001 the amount of the main network) and this also minimizes training oscillations and divergence.\n",
    "\n",
    "### Summary of hyperparameters\n",
    "\n",
    "- γ (discount rate): .99\n",
    "- α (learning rate): 5e-4\n",
    "- τ (soft update rate for target network): .001\n",
    "- Batch size: 64\n",
    "- Network updates every 4 steps\n",
    "\n",
    "## Plot of rewards\n",
    "![plot.png](plot.png)\n",
    "\n",
    "The environment was solved in 519 episodes with an average score of 13.00.\n",
    "\n",
    "## Ideas for future work\n",
    "\n",
    "There are many different ways the current implementation can be improved:\n",
    "- Prioritized experience replay can be used to assign a priority to each experience in the replay buffer based on TD error. This will allow more important experiences to be utilized more frequently.\n",
    "- Dueling networks can be used to learn the state values in one network and the advantage values in another, with the networks able to share some layers. This has been shown to lead to better policy evaluation when many actions have similar values, such as in this environment.\n",
    "- A combination of these methods can be used, also known as a rainbow method. Other extensions can also be incorporated, such as multi-step bootstrap targets, distributional DQN, and noisy DQN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
